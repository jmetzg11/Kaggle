{"cells":[{"metadata":{},"cell_type":"markdown","source":"# New York City Taxi Trip Duration\n- goal is to predict duration of trip\n- inspired by GrandMaster Mr. Beluga's [notebook](https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import timedelta\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = [16, 10]\n\ntrain = pd.read_csv('../input/nyc-taxi-trip-duration/train.zip')\ntest = pd.read_csv('../input/nyc-taxi-trip-duration/test.zip')\nsample_submission = pd.read_csv('../input/nyc-taxi-trip-duration/sample_submission.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of samples:', train.shape[0])\nprint('Number of numerical features:', train.select_dtypes(include=['int64','float64']).shape[1])\nprint('Number of categorical featrues:', train.select_dtypes(include='object').shape[1])\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are unique values for id:',train.id.nunique()==train.shape[0])\nprint('There is no missing values:',train.count().min() == train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first=set(train.columns)\nsecond=set(test.columns)\nprint('Missing columns from test set:', first-second)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Chaging string value into integer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['store_and_fwd_flag']=[0 if x=='N' else 1 for x in train['store_and_fwd_flag']]\ntest['store_and_fwd_flag']=[0 if x=='N' else 1 for x in test['store_and_fwd_flag']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Changing to datetime values\n- Changing to pandas datetime\n- adding a minute, hour, day, week hour and month column to train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing to datetime\ntrain['pickup_datetime']=pd.to_datetime(train.pickup_datetime)\ntrain['dropoff_datetime']=pd.to_datetime(train.dropoff_datetime)\ntest['pickup_datetime']=pd.to_datetime(test.pickup_datetime)\n\n#adding minute column\ntrain['minute']=train.pickup_datetime.dt.minute\ntest['minute']=train.pickup_datetime.dt.minute\n\n# adding hour column\ntrain['hour']=train.pickup_datetime.dt.hour\ntest['hour']=test.pickup_datetime.dt.hour\n\n# adding day column\ntrain['day']=train.pickup_datetime.dt.weekday\ntest['day']=train.pickup_datetime.dt.weekday\n\n# adding week hour column\ntrain['week_hour']=train['day']*24+train['hour']\ntest['week_hour']=test['day']*24+test['hour']\n\n# adding month column\ntrain['month']=train.pickup_datetime.dt.month\ntest['month']=train.pickup_datetime.dt.month\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No need to look for yearly cyclical patterns because the sample are from a 5 month period."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Time frame of the data set:', train.month.max()-train.month.min(), 'months')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of time values\n- Day of the week is nearly uniformally distributed\n- Hour of day has a distinct patter\n- More observations are made in the last month"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(3,2)\n\n# hour\nax[0,0].hist(train.hour, bins=24)                 \nax[0,0].set_xlabel('Hours in Train')\nax[0,1].hist(test.hour, bins=24)\nax[0,1].set_xlabel('Hours in Test')\n\n# day\nax[1,0].hist(train.day, bins=7)                   \nax[1,0].set_xlabel('Days in Test')\nax[1,1].hist(test.day, bins=7)\nax[1,1].set_xlabel('Days in Test')\n\n# month\nax[2,0].hist(test.month, bins=5)                \nax[2,0].set_xlabel('Months in Train')\nax[2,1].hist(test.month, bins=5)\nax[2,1].set_xlabel('Months in Train')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of the target vairable\n- Raw data is very skewed due to some very long trips. The variable resembles normal after a log transformation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log_duration']=np.log(train['trip_duration'])\nfig, ax = plt.subplots(2,2)\nax[0,0].hist(train['trip_duration'],bins=100)\nax[0,0].set_xlabel('Trip duration')\nax[0,1].hist(train['log_duration'],bins=100)\nax[0,1].set_xlabel('Log trip duraiton')\nax[1,0].boxplot(train['trip_duration'])\nax[1,0].set_xlabel('Trip duration')\nax[1,1].boxplot(train['log_duration'])\nax[1,1].set_xlabel('Log trip duration')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction\n- inspired by Nir Malbin's [notebook](https://www.kaggle.com/donniedarko/darktaxi-tripdurationprediction-lb-0-385)\n- This will transform the coordinates. The idea is that our trees will have more features to work with. It's proved to a successful technique in other kernels "},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking all the coordinates from the training and test set\ncoords=np.vstack((train[['pickup_latitude','pickup_longitude']], train[['pickup_latitude','pickup_longitude']],\n                 test[['pickup_latitude','pickup_longitude']], test[['pickup_latitude','pickup_longitude']]))\n\n# Keeping coordinates that fall within one standard deviation\nmin_lat, min_lng = coords.mean(axis=0)-coords.std(axis=0)\nmax_lat, max_lng = coords.mean(axis=0)+coords.std(axis=0)\ncoords=coords[(coords[:,0]>min_lat) & (coords[:,0]<max_lat) & (coords[:,1] > min_lng) & (coords[:,1] < max_lng)]\n\n# PCA transformation\npca=PCA().fit(coords)\ncoords_pca=pca.transform(coords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- visualing the new features compared to the old"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 1)\nsample_ind = np.random.permutation(len(coords))[:10000]\nax[0].scatter(coords[sample_ind,0], coords[sample_ind,1], s=1, lw=0)\nax[0].set_title('Original')\nax[1].scatter(coords_pca[sample_ind,0], coords_pca[sample_ind,1], s=1, lw=0)\nax[1].set_title('After PCA')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- adding new features to training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train pickup\ntrain['pickup_pca0']=pca.transform(train[['pickup_latitude','pickup_longitude']])[:,0]  \ntrain['pickup_pca1']=pca.transform(train[['pickup_latitude','pickup_longitude']])[:,1]\n\n# train dropoff\ntrain['dropoff_pca0']=pca.transform(train[['dropoff_latitude','dropoff_longitude']])[:,0]    \ntrain['dropoff_pca1']=pca.transform(train[['dropoff_latitude','dropoff_longitude']])[:,1]\n\n# train pickup\ntest['pickup_pca0']=pca.transform(test[['pickup_latitude','pickup_longitude']])[:,0]   \ntest['pickup_pca1']=pca.transform(test[['pickup_latitude','pickup_longitude']])[:,1]\n\n# train dropoff\ntest['dropoff_pca0']=pca.transform(test[['dropoff_latitude','dropoff_longitude']])[:,0]    \ntest['dropoff_pca1']=pca.transform(test[['dropoff_latitude','dropoff_longitude']])[:,1]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distance and direction variables\n- Manhattan method with PCA values = abs(dropoff-pickup)\n- the Haversine formula is an accurate measurement on popular to calcuate distances spherical surfaces. \\\n$$distance = 2r*arcsin\\sqrt{sin^{2}\\frac{lat2-lat1}{2}+cos(lat1)cos(lat2)sin^{2}\\frac{lng2-lng1}{2}}$$ <br> where r is the radius of the earth at New York\n- we can also find the direction with arctan2(x,y) <br> where x = sin(lng2-lng1)*cos(lat2) and y = cos(lat1)sin(lat2)-sin(lat1)cos(lat2)cos(lng2-lng1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to creat harvsin distance\ndef harvsin(lat1,lng1,lat2,lng2):\n    radius=6369\n    lat1,lng1,lat2,lng2=map(np.radians,(lat1,lng1,lat2,lng2))\n    lat=lat2-lat1\n    lng=lng2-lng1\n    inside=(np.sin(lat*0.5)**2)+np.cos(lat1)*np.cos(lat2)*(np.sin(lng*0.5)**2)\n    return 2*radius*np.arcsin(np.sqrt(inside))\n\n# function to determine distance\ndef bearing(lat1, lng1, lat2, lng2):\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\n# adding Manhattan distance, harvsine distance, and direction to training set\ntrain['pca_manhattan']=(train['dropoff_pca0']-train['pickup_pca0']).abs()+(train['dropoff_pca1']-train['pickup_pca1']).abs()\ntrain['harvsin']=harvsin(train['pickup_latitude'], train['pickup_longitude'], train['dropoff_latitude'], train['dropoff_longitude'])\ntest['direction']=bearing(train['pickup_latitude'], test['pickup_longitude'], test['dropoff_latitude'], test['dropoff_longitude'])\n\n# adding Manhattan distance, harvsine distance, and direction to test set \ntest['pca_manhattan']=(test['dropoff_pca0']-test['pickup_pca0']).abs()+(test['dropoff_pca1']-test['pickup_pca1']).abs()\ntest['harvsin']=harvsin(test['pickup_latitude'], test['pickup_longitude'], test['dropoff_latitude'], test['dropoff_longitude'])\ntest['direction']=bearing(test['pickup_latitude'], test['pickup_longitude'], test['dropoff_latitude'], test['dropoff_longitude'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Speed"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['harvsin_speed']=1000*train['harvsin']/train['trip_duration']\ntrain['manhattan_speed']=1000*train['pca_manhattan']/train['trip_duration']\n\nfig, ax = plt.subplots(1,3,sharey=True, figsize=(16,6))\nax[0].plot(train.groupby('hour').mean()['harvsin_speed'],'bo--',lw=2,alpha=.4)\nax[0].set_xlabel('hour')\nax[1].plot(train.groupby('day').mean()['harvsin_speed'],'go--',lw=2,alpha=.4)\nax[1].set_xlabel('day')\nax[2].plot(train.groupby('week_hour').mean()['harvsin_speed'],'ro--',lw=2,alpha=.4)\nax[2].set_xlabel('week hour')\nfig.suptitle('Speed Patterns')\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering by pickup and dropoff\n- using the previous trimmed down coordinates to make cluster and traffic count into and out of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = MiniBatchKMeans(n_clusters=8**2, batch_size=32**3).fit(coords)\nsample_ind=np.random.permutation(len(coords))[:10000]\nplt.scatter(coords[sample_ind,0],coords[sample_ind,1],s=1,lw=0,c=kmeans.predict(coords[sample_ind]), cmap='tab20')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['pickup_cluster']=kmeans.predict(train[['pickup_latitude','pickup_longitude']])\ntrain['dropoff_cluster']=kmeans.predict(train[['dropoff_latitude','dropoff_latitude']])\n\ntest['pickup_cluster']=kmeans.predict(test[['pickup_latitude','pickup_longitude']])\ntest['dropoff_cluster']=kmeans.predict(test[['dropoff_latitude','dropoff_latitude']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combining training and test in order to count movement in and out of clusters\ndf_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\n\ntrain['pickup_datetime_group'] = train['pickup_datetime'].dt.round('60min')\ntest['pickup_datetime_group'] = test['pickup_datetime'].dt.round('60min')\n\n# count dropoff into clusters\ndf_dropoff_counts = df_all.set_index('pickup_datetime').groupby([pd.Grouper(freq='60min'), 'dropoff_cluster']).agg({'id': 'count'}) \\\n                   .reset_index().set_index('pickup_datetime').groupby('dropoff_cluster').rolling('240min').mean().drop('dropoff_cluster', axis=1) \\\n                   .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n                   .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'dropoff_cluster_count'})\n\n# count pickups from clusters\ndf_pickup_counts = df_all.set_index('pickup_datetime').groupby([pd.Grouper(freq='60min'), 'pickup_cluster']).agg({'id': 'count'}) \\\n                  .reset_index().set_index('pickup_datetime').groupby('pickup_cluster').rolling('240min').mean().drop('pickup_cluster', axis=1) \\\n                  .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n                  .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'pickup_cluster_count'})\n\n# adding counts into training\ntrain['dropoff_cluster_count'] = train[['pickup_datetime_group', 'dropoff_cluster']].merge(df_dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\ntrain['pickup_cluster_count'] = train[['pickup_datetime_group', 'pickup_cluster']].merge(df_pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)\n\n# adding counts into test\ntest['dropoff_cluster_count'] = test[['pickup_datetime_group', 'dropoff_cluster']].merge(df_dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\ntest['pickup_cluster_count'] = test[['pickup_datetime_group', 'pickup_cluster']].merge(df_pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Supplamentry [data](https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm)\n- someone went through the trouble to extract the fastest routs. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fr1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv',usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps'])\nfr2 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_2.csv',usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntest_street_info = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv',usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntrain_street_info = pd.concat((fr1, fr2))\ntrain = train.merge(train_street_info, how='left', on='id')\ntest = test.merge(test_street_info, how='left', on='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting features and correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"exclude = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime','trip_duration','log_duration',\n           'pickup_datetime_group','harvsin_speed','manhattan_speed']\nfeatures=[x for x in train.columns if x not in exclude]\ny_train=np.log(train['trip_duration'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,9))\nsns.heatmap(train.corr()[['log_duration']].sort_values('log_duration'), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model with validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 20% is set aside as a validation set\nX_train, X_val, y_train, y_val = train_test_split(train[features].values,y_train,test_size=.2)\n\n# Data Matrix form to memory efficiency and training speed\ntrain_matrix=xgb.DMatrix(X_train,label=y_train)\nval_matrix=xgb.DMatrix(X_val,label=y_val)\ntest_matrix=xgb.DMatrix(test[features].values)\n\n# using previously found parameters\nxgb_pars=xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n            'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n\nwatchlist = [(train_matrix, 'train'), (val_matrix, 'valid')]\n\nmodel = xgb.train(xgb_pars, train_matrix, 100, watchlist, early_stopping_rounds=20,\n                  maximize=False, verbose_eval=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = model.predict(test_matrix)\ntest['trip_duration']=np.exp(y_test)\ntest[['id', 'trip_duration']].to_csv('ny_taxi_submission.csv.gz', index=False, compression='gzip')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}